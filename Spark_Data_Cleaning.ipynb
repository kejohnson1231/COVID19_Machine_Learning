{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "spark = (ps.sql.SparkSession.builder \n",
    "        .master(\"local[4]\") \n",
    "        .appName(\"sparkSQL exercise\") \n",
    "        .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json             # for parsing json formatted data\n",
    "import csv              # for the split_csvstring function from Part 3.2.2\n",
    "try:                    # Python 3 compatibility\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "import os  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/US_counties_COVID19_health_weather_data.csv',\n",
    "                   header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 146.0 failed 1 times, most recent failure: Lost task 0.0 in stage 146.0 (TID 191, localhost): java.io.FileNotFoundException: File file:/home/jovyan/work/Desktop/Galvanize/Capstones/COVID19_Machine_Learning/data/US_counties_COVID19_health_weather_data.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:119)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply$mcI$sp(EvaluatePython.scala:41)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.takeAndServe(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(EvaluatePython.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: File file:/home/jovyan/work/Desktop/Galvanize/Capstones/COVID19_Machine_Learning/data/US_counties_COVID19_health_weather_data.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:119)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-b7bfb4d1bc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             port = self._sc._jvm.org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(\n\u001b[0;32m--> 350\u001b[0;31m                 self._jdf, num)\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 146.0 failed 1 times, most recent failure: Lost task 0.0 in stage 146.0 (TID 191, localhost): java.io.FileNotFoundException: File file:/home/jovyan/work/Desktop/Galvanize/Capstones/COVID19_Machine_Learning/data/US_counties_COVID19_health_weather_data.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:119)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply$mcI$sp(EvaluatePython.scala:41)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.takeAndServe(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(EvaluatePython.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: File file:/home/jovyan/work/Desktop/Galvanize/Capstones/COVID19_Machine_Learning/data/US_counties_COVID19_health_weather_data.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:119)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interested_list = ['date','county','state','fips','population_density_per_sqmi','percent_fair_or_poor_health',\n",
    "                   'percent_smokers','percent_adults_with_obesity','percent_physically_inactive',\n",
    "                   'percent_with_access_to_exercise_opportunities','percent_uninsured',\n",
    "                   'primary_care_physicians_rate','percent_vaccinated','life_expectancy',\n",
    "                   'percent_adults_with_diabetes','percent_limited_access_to_healthy_foods',\n",
    "                   'percent_insufficient_sleep','percent_uninsured_2','percent_uninsured_3',\n",
    "                   'other_primary_care_provider_rate','median_household_income','segregation_index',\n",
    "                   'segregation_index_2','percent_less_than_18_years_of_age',\n",
    "                   'percent_65_and_over','percent_black','percent_american_indian_alaska_native','percent_asian',\n",
    "                   'percent_native_hawaiian_other_pacific_islander','percent_hispanic','percent_non_hispanic_white',\n",
    "                   'percent_not_proficient_in_english','percent_female','percent_rural','per_capita_income',\n",
    "                   'percent_below_poverty','percent_unemployed_CDC','percent_age_65_and_older',\n",
    "                   'percent_age_17_and_younger','percent_minorities', 'percent_disabled'\n",
    "                   'percent_multi_unit_housing', 'km_to_closest_station',\n",
    "                   'station_name','mean_temp','date_stay_at_home_announced','date_stay_at_home_effective']\n",
    "\n",
    "count_col = len(interested_list)\n",
    "count_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interested_list = ['date','county','state','fips','population_density_per_sqmi','percent_fair_or_poor_health',\n",
    "                   'percent_smokers','percent_adults_with_obesity','percent_physically_inactive',\n",
    "                   'percent_uninsured','primary_care_physicians_rate','percent_vaccinated','life_expectancy',\n",
    "                   'percent_adults_with_diabetes','percent_insufficient_sleep',\n",
    "                   'other_primary_care_provider_rate','median_household_income',\n",
    "                   'segregation_index_2','percent_less_than_18_years_of_age','percent_65_and_over',\n",
    "                   'percent_not_proficient_in_english','percent_female','percent_rural','per_capita_income',\n",
    "                   'percent_below_poverty','percent_age_65_and_older', 'percent_disabled'\n",
    "                   'percent_age_17_and_younger','percent_minorities',\n",
    "                   'percent_multi_unit_housing', 'mean_temp',\n",
    "                   'date_stay_at_home_announced','date_stay_at_home_effective']\n",
    "\n",
    "count_col = len(interested_list)\n",
    "count_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date, county, state, fips, population_density_per_sqmi, percent_fair_or_poor_health, percent_smokers, percent_adults_with_obesity, percent_physically_inactive, percent_uninsured, primary_care_physicians_rate, percent_vaccinated, life_expectancy, percent_adults_with_diabetes, percent_insufficient_sleep, other_primary_care_provider_rate, median_household_income, segregation_index_2, percent_less_than_18_years_of_age, percent_65_and_over, percent_not_proficient_in_english, percent_female, percent_rural, per_capita_income, percent_below_poverty, percent_age_65_and_older, percent_disabledpercent_age_17_and_younger, percent_minorities, percent_multi_unit_housing, mean_temp, date_stay_at_home_announced, date_stay_at_home_effective'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_to_list(lst):\n",
    "    string = ''\n",
    "    for i in lst[0:(len(lst)-1)]:\n",
    "        string += i\n",
    "        string += ', '\n",
    "    for i in lst[len(lst)-1]:\n",
    "        string += i\n",
    "    return string\n",
    "\n",
    "string_to_list(interested_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('data/US_counties_COVID19_health_weather_data.csv')\n",
    "\n",
    "df.createOrReplaceTempView('covid1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_query = '''\n",
    "SELECT date, county, state, fips, population_density_per_sqmi, percent_fair_or_poor_health, \n",
    "percent_smokers, percent_adults_with_obesity, percent_physically_inactive, \n",
    "percent_with_access_to_exercise_opportunities, percent_uninsured, primary_care_physicians_rate, \n",
    "percent_vaccinated, life_expectancy, percent_adults_with_diabetes, percent_disabled,\n",
    "percent_limited_access_to_healthy_foods, percent_insufficient_sleep, percent_uninsured_2, \n",
    "percent_uninsured_3, other_primary_care_provider_rate, median_household_income, segregation_index,\n",
    "segregation_index_2, percent_less_than_18_years_of_age, percent_65_and_over, percent_black, \n",
    "percent_american_indian_alaska_native, percent_asian, percent_native_hawaiian_other_pacific_islander, \n",
    "percent_hispanic, percent_non_hispanic_white, percent_not_proficient_in_english, percent_female, \n",
    "percent_rural, per_capita_income, percent_below_poverty, percent_unemployed_CDC, percent_age_65_and_older, \n",
    "percent_age_17_and_younger, percent_minorities, percent_multi_unit_housing, percent_overcrowding, \n",
    "km_to_closest_station, station_name, mean_temp, date_stay_at_home_announced, date_stay_at_home_effective\n",
    "FROM covid1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "county = date, county, state, fips\n",
    "\n",
    "\n",
    "fips +\n",
    " \n",
    "financial = per_capita_income, percent_below_poverty, percent_unemployed_CDC, median_household_income,\n",
    "\n",
    "age = percent_less_than_18_years_of_age, percent_65_and_over, percent_age_65_and_older, percent_age_17_and_younger,\n",
    "\n",
    "population density = population_density_per_sqmi, percent_rural, percent_multi_unit_housing, percent_overcrowding, \n",
    "\n",
    "access = percent_uninsured, primary_care_physicians_rate,  percent_uninsured_2, \n",
    "percent_uninsured_3, other_primary_care_provider_rate, percent_not_proficient_in_english\n",
    "\n",
    "health = percent_vaccinated, percent_adults_with_diabetes, percent_physically_inactive, percent_adults_with_obesity, \n",
    "percent_smokers, percent_limited_access_to_healthy_foods, percent_insufficient_sleep, life_expectancy, percent_with_access_to_exercise_opportunities, percent_fair_or_poor_health, \n",
    "\n",
    "demographic = percent_black, percent_american_indian_alaska_native, percent_asian, percent_native_hawaiian_other_pacific_islander, percent_hispanic, percent_non_hispanic_white, percent_female, segregation_index, segregation_index_2, percent_minorities\n",
    "\n",
    "weather = km_to_closest_station, station_name, mean_temp, \n",
    "\n",
    "stay at home = date_stay_at_home_announced, date_stay_at_home_effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_df1 = spark.sql(full_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----+\n",
      "|      date|     county|     state| fips|\n",
      "+----------+-----------+----------+-----+\n",
      "|2020-01-21|  Snohomish|Washington|53061|\n",
      "|2020-01-22|  Snohomish|Washington|53061|\n",
      "|2020-01-23|  Snohomish|Washington|53061|\n",
      "|2020-01-24|       Cook|  Illinois|17031|\n",
      "|2020-01-24|  Snohomish|Washington|53061|\n",
      "|2020-01-25|     Orange|California|06059|\n",
      "|2020-01-25|       Cook|  Illinois|17031|\n",
      "|2020-01-25|  Snohomish|Washington|53061|\n",
      "|2020-01-26|   Maricopa|   Arizona|04013|\n",
      "|2020-01-26|Los Angeles|California|06037|\n",
      "|2020-01-26|     Orange|California|06059|\n",
      "|2020-01-26|       Cook|  Illinois|17031|\n",
      "|2020-01-26|  Snohomish|Washington|53061|\n",
      "|2020-01-27|   Maricopa|   Arizona|04013|\n",
      "|2020-01-27|Los Angeles|California|06037|\n",
      "|2020-01-27|     Orange|California|06059|\n",
      "|2020-01-27|       Cook|  Illinois|17031|\n",
      "|2020-01-27|  Snohomish|Washington|53061|\n",
      "|2020-01-28|   Maricopa|   Arizona|04013|\n",
      "|2020-01-28|Los Angeles|California|06037|\n",
      "+----------+-----------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "county_query = 'SELECT date, county, state, fips FROM covid1'\n",
    "\n",
    "county_df = spark.sql(county_query)\n",
    "\n",
    "county_df.createOrReplaceTempView('county')\n",
    "\n",
    "county_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+---------------+--------------------+\n",
      "| fips|perc_below_poverty_line|perc_unemployed|med_household_income|\n",
      "+-----+-----------------------+---------------+--------------------+\n",
      "|53061|                    9.3|            6.2|               87096|\n",
      "|53061|                    9.3|            6.2|               87096|\n",
      "|53061|                    9.3|            6.2|               87096|\n",
      "|17031|                   16.7|            9.7|               63347|\n",
      "|53061|                    9.3|            6.2|               87096|\n",
      "|06059|                   12.5|            6.8|               89373|\n",
      "|17031|                   16.7|            9.7|               63347|\n",
      "|53061|                    9.3|            6.2|               87096|\n",
      "|04013|                   16.5|            6.8|               65234|\n",
      "|06037|                   17.8|            8.9|               67986|\n",
      "|06059|                   12.5|            6.8|               89373|\n",
      "|17031|                   16.7|            9.7|               63347|\n",
      "|53061|                    9.3|            6.2|               87096|\n",
      "|04013|                   16.5|            6.8|               65234|\n",
      "|06037|                   17.8|            8.9|               67986|\n",
      "|06059|                   12.5|            6.8|               89373|\n",
      "|17031|                   16.7|            9.7|               63347|\n",
      "|53061|                    9.3|            6.2|               87096|\n",
      "|04013|                   16.5|            6.8|               65234|\n",
      "|06037|                   17.8|            8.9|               67986|\n",
      "+-----+-----------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "financial_query = '''\n",
    "SELECT fips, \n",
    "percent_below_poverty as perc_below_poverty_line, \n",
    "percent_unemployed_CDC as perc_unemployed, median_household_income as med_household_income\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "financial_df = spark.sql(financial_query)\n",
    "\n",
    "financial_df.createOrReplaceTempView('financial')\n",
    "\n",
    "financial_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------------+----------------+-----------------+\n",
      "| fips|p_less_than_18|p_17_younger|perc_65_and_over|perc_65_and_older|\n",
      "+-----+--------------+------------+----------------+-----------------+\n",
      "|53061|  22.646554612|        23.2|    13.470102503|             12.1|\n",
      "|53061|  22.646554612|        23.2|    13.470102503|             12.1|\n",
      "|53061|  22.646554612|        23.2|    13.470102503|             12.1|\n",
      "|17031|  21.806264384|        22.7|      14.6350357|               13|\n",
      "|53061|  22.646554612|        23.2|    13.470102503|             12.1|\n",
      "|06059|  21.933302532|          23|    14.764335361|             13.2|\n",
      "|17031|  21.806264384|        22.7|      14.6350357|               13|\n",
      "|53061|  22.646554612|        23.2|    13.470102503|             12.1|\n",
      "|04013|  23.868284021|          25|    15.173695436|             13.8|\n",
      "|06037|  21.660374065|        22.8|     13.61589777|             12.2|\n",
      "|06059|  21.933302532|          23|    14.764335361|             13.2|\n",
      "|17031|  21.806264384|        22.7|      14.6350357|               13|\n",
      "|53061|  22.646554612|        23.2|    13.470102503|             12.1|\n",
      "|04013|  23.868284021|          25|    15.173695436|             13.8|\n",
      "|06037|  21.660374065|        22.8|     13.61589777|             12.2|\n",
      "|06059|  21.933302532|          23|    14.764335361|             13.2|\n",
      "|17031|  21.806264384|        22.7|      14.6350357|               13|\n",
      "|53061|  22.646554612|        23.2|    13.470102503|             12.1|\n",
      "|04013|  23.868284021|          25|    15.173695436|             13.8|\n",
      "|06037|  21.660374065|        22.8|     13.61589777|             12.2|\n",
      "+-----+--------------+------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "age_query = '''\n",
    "SELECT fips, \n",
    "percent_less_than_18_years_of_age AS p_less_than_18, \n",
    "percent_age_17_and_younger as p_17_younger,\n",
    "percent_65_and_over as perc_65_and_over,\n",
    "percent_age_65_and_older as perc_65_and_older\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "age_df = spark.sql(age_query)\n",
    "age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "youngerColumns = [col('p_less_than_18'), col('p_17_younger')]\n",
    "averageFunc = sum(x for x in youngerColumns)/len(youngerColumns)\n",
    "age_df = age_df.withColumn('p_17_younger', averageFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "olderColumns = [col('perc_65_and_over'), col('perc_65_and_older')]\n",
    "averageFunc = sum(x for x in olderColumns)/len(olderColumns)\n",
    "age_df = age_df.withColumn('p_65_older', averageFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------------------+----------------+-----------------+-------------+\n",
      "| fips|p_less_than_18|      p_17_younger|perc_65_and_over|perc_65_and_older|   p_65_older|\n",
      "+-----+--------------+------------------+----------------+-----------------+-------------+\n",
      "|53061|  22.646554612|      22.923277306|    13.470102503|             12.1|12.7850512515|\n",
      "|53061|  22.646554612|      22.923277306|    13.470102503|             12.1|12.7850512515|\n",
      "|53061|  22.646554612|      22.923277306|    13.470102503|             12.1|12.7850512515|\n",
      "|17031|  21.806264384|      22.253132192|      14.6350357|               13|  13.81751785|\n",
      "|53061|  22.646554612|      22.923277306|    13.470102503|             12.1|12.7850512515|\n",
      "|06059|  21.933302532|      22.466651266|    14.764335361|             13.2|13.9821676805|\n",
      "|17031|  21.806264384|      22.253132192|      14.6350357|               13|  13.81751785|\n",
      "|53061|  22.646554612|      22.923277306|    13.470102503|             12.1|12.7850512515|\n",
      "|04013|  23.868284021|     24.4341420105|    15.173695436|             13.8| 14.486847718|\n",
      "|06037|  21.660374065|22.230187032499998|     13.61589777|             12.2| 12.907948885|\n",
      "|06059|  21.933302532|      22.466651266|    14.764335361|             13.2|13.9821676805|\n",
      "|17031|  21.806264384|      22.253132192|      14.6350357|               13|  13.81751785|\n",
      "|53061|  22.646554612|      22.923277306|    13.470102503|             12.1|12.7850512515|\n",
      "|04013|  23.868284021|     24.4341420105|    15.173695436|             13.8| 14.486847718|\n",
      "|06037|  21.660374065|22.230187032499998|     13.61589777|             12.2| 12.907948885|\n",
      "|06059|  21.933302532|      22.466651266|    14.764335361|             13.2|13.9821676805|\n",
      "|17031|  21.806264384|      22.253132192|      14.6350357|               13|  13.81751785|\n",
      "|53061|  22.646554612|      22.923277306|    13.470102503|             12.1|12.7850512515|\n",
      "|04013|  23.868284021|     24.4341420105|    15.173695436|             13.8| 14.486847718|\n",
      "|06037|  21.660374065|22.230187032499998|     13.61589777|             12.2| 12.907948885|\n",
      "+-----+--------------+------------------+----------------+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_df.createOrReplaceTempView('ages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-------------+\n",
      "| fips|      p_17_younger|   p_65_older|\n",
      "+-----+------------------+-------------+\n",
      "|53061|      22.923277306|12.7850512515|\n",
      "|53061|      22.923277306|12.7850512515|\n",
      "|53061|      22.923277306|12.7850512515|\n",
      "|17031|      22.253132192|  13.81751785|\n",
      "|53061|      22.923277306|12.7850512515|\n",
      "|06059|      22.466651266|13.9821676805|\n",
      "|17031|      22.253132192|  13.81751785|\n",
      "|53061|      22.923277306|12.7850512515|\n",
      "|04013|     24.4341420105| 14.486847718|\n",
      "|06037|22.230187032499998| 12.907948885|\n",
      "|06059|      22.466651266|13.9821676805|\n",
      "|17031|      22.253132192|  13.81751785|\n",
      "|53061|      22.923277306|12.7850512515|\n",
      "|04013|     24.4341420105| 14.486847718|\n",
      "|06037|22.230187032499998| 12.907948885|\n",
      "|06059|      22.466651266|13.9821676805|\n",
      "|17031|      22.253132192|  13.81751785|\n",
      "|53061|      22.923277306|12.7850512515|\n",
      "|04013|     24.4341420105| 14.486847718|\n",
      "|06037|22.230187032499998| 12.907948885|\n",
      "+-----+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "age2_query = '''\n",
    "SELECT fips, p_17_younger, p_65_older\n",
    "FROM ages\n",
    "'''\n",
    "\n",
    "age_df = spark.sql(age2_query)\n",
    "age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_df.createOrReplaceTempView('ages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+------------------+\n",
      "| fips|pop_density_per_sqmi|     p_rural|p_multi_unit_house|\n",
      "+-----+--------------------+------------+------------------+\n",
      "|53061|   363.5861592633876|10.819460702|              14.6|\n",
      "|53061|   363.5861592633876|10.819460702|              14.6|\n",
      "|53061|   363.5861592633876|10.819460702|              14.6|\n",
      "|17031|   5531.878538800693|0.0452963852|                23|\n",
      "|53061|   363.5861592633876|10.819460702|              14.6|\n",
      "|06059|   3961.705072130077|0.1433444333|              18.7|\n",
      "|17031|   5531.878538800693|0.0452963852|                23|\n",
      "|53061|   363.5861592633876|10.819460702|              14.6|\n",
      "|04013|   444.4467438038513|2.3637996949|              14.6|\n",
      "|06037|  2478.2406446761356|0.6052183584|              26.5|\n",
      "|06059|   3961.705072130077|0.1433444333|              18.7|\n",
      "|17031|   5531.878538800693|0.0452963852|                23|\n",
      "|53061|   363.5861592633876|10.819460702|              14.6|\n",
      "|04013|   444.4467438038513|2.3637996949|              14.6|\n",
      "|06037|  2478.2406446761356|0.6052183584|              26.5|\n",
      "|06059|   3961.705072130077|0.1433444333|              18.7|\n",
      "|17031|   5531.878538800693|0.0452963852|                23|\n",
      "|53061|   363.5861592633876|10.819460702|              14.6|\n",
      "|04013|   444.4467438038513|2.3637996949|              14.6|\n",
      "|06037|  2478.2406446761356|0.6052183584|              26.5|\n",
      "+-----+--------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_dens_query = '''\n",
    "SELECT  fips, \n",
    "population_density_per_sqmi as pop_density_per_sqmi, percent_rural as p_rural,\n",
    "percent_multi_unit_housing as multi_unit_housing\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "pop_dens_df = spark.sql(pop_dens_query)\n",
    "\n",
    "pop_dens_df.createOrReplaceTempView('pop_dens')\n",
    "pop_dens_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+---------+--------------+\n",
      "| fips|     p_unins|pcp_phys_r|pcp_other|p_not_eng_prof|\n",
      "+-----+------------+----------+---------+--------------+\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|\n",
      "|17031| 9.972268492|  93.31711| 81.67176|   7.028651671|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|\n",
      "|06059|8.3365759293|  97.41725| 61.20589|  10.249305431|\n",
      "|17031| 9.972268492|  93.31711| 81.67176|   7.028651671|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|\n",
      "|04013|12.081175995|  70.35005|105.71721|  5.1505854956|\n",
      "|06037|10.059485018|     72.78|   60.937|  13.691354033|\n",
      "|06059|8.3365759293|  97.41725| 61.20589|  10.249305431|\n",
      "|17031| 9.972268492|  93.31711| 81.67176|   7.028651671|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|\n",
      "|04013|12.081175995|  70.35005|105.71721|  5.1505854956|\n",
      "|06037|10.059485018|     72.78|   60.937|  13.691354033|\n",
      "|06059|8.3365759293|  97.41725| 61.20589|  10.249305431|\n",
      "|17031| 9.972268492|  93.31711| 81.67176|   7.028651671|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|\n",
      "|04013|12.081175995|  70.35005|105.71721|  5.1505854956|\n",
      "|06037|10.059485018|     72.78|   60.937|  13.691354033|\n",
      "+-----+------------+----------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "access_query = '''\n",
    "SELECT fips, \n",
    "percent_uninsured as p_unins, primary_care_physicians_rate as pcp_phys_r, \n",
    "other_primary_care_provider_rate as pcp_other, \n",
    "percent_not_proficient_in_english as p_not_eng_prof\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "access_df = spark.sql(access_query)\n",
    "access_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+---------+--------------+------------------+\n",
      "| fips|     p_unins|pcp_phys_r|pcp_other|p_not_eng_prof|         PCP_ratio|\n",
      "+-----+------------+----------+---------+--------------+------------------+\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|         112.52509|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|         112.52509|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|         112.52509|\n",
      "|17031| 9.972268492|  93.31711| 81.67176|   7.028651671|174.98887000000002|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|         112.52509|\n",
      "|06059|8.3365759293|  97.41725| 61.20589|  10.249305431|158.62313999999998|\n",
      "|17031| 9.972268492|  93.31711| 81.67176|   7.028651671|174.98887000000002|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|         112.52509|\n",
      "|04013|12.081175995|  70.35005|105.71721|  5.1505854956|176.06725999999998|\n",
      "|06037|10.059485018|     72.78|   60.937|  13.691354033|133.71699999999998|\n",
      "|06059|8.3365759293|  97.41725| 61.20589|  10.249305431|158.62313999999998|\n",
      "|17031| 9.972268492|  93.31711| 81.67176|   7.028651671|174.98887000000002|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|         112.52509|\n",
      "|04013|12.081175995|  70.35005|105.71721|  5.1505854956|176.06725999999998|\n",
      "|06037|10.059485018|     72.78|   60.937|  13.691354033|133.71699999999998|\n",
      "|06059|8.3365759293|  97.41725| 61.20589|  10.249305431|158.62313999999998|\n",
      "|17031| 9.972268492|  93.31711| 81.67176|   7.028651671|174.98887000000002|\n",
      "|53061|6.2076401725|   52.5178| 60.00729|  3.5068929756|         112.52509|\n",
      "|04013|12.081175995|  70.35005|105.71721|  5.1505854956|176.06725999999998|\n",
      "|06037|10.059485018|     72.78|   60.937|  13.691354033|133.71699999999998|\n",
      "+-----+------------+----------+---------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcpColumns = [col('pcp_phys_r'), col('pcp_other')]\n",
    "sumFunc = sum(x for x in pcpColumns)\n",
    "access_df = access_df.withColumn('PCP_ratio', sumFunc)\n",
    "access_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_df.createOrReplaceTempView('access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-----------------------+------------------+\n",
      "| fips|  perc_unins|perc_not_eng_proficient|         PCP_ratio|\n",
      "+-----+------------+-----------------------+------------------+\n",
      "|53061|6.2076401725|           3.5068929756|         112.52509|\n",
      "|53061|6.2076401725|           3.5068929756|         112.52509|\n",
      "|53061|6.2076401725|           3.5068929756|         112.52509|\n",
      "|17031| 9.972268492|            7.028651671|174.98887000000002|\n",
      "|53061|6.2076401725|           3.5068929756|         112.52509|\n",
      "|06059|8.3365759293|           10.249305431|158.62313999999998|\n",
      "|17031| 9.972268492|            7.028651671|174.98887000000002|\n",
      "|53061|6.2076401725|           3.5068929756|         112.52509|\n",
      "|04013|12.081175995|           5.1505854956|176.06725999999998|\n",
      "|06037|10.059485018|           13.691354033|133.71699999999998|\n",
      "|06059|8.3365759293|           10.249305431|158.62313999999998|\n",
      "|17031| 9.972268492|            7.028651671|174.98887000000002|\n",
      "|53061|6.2076401725|           3.5068929756|         112.52509|\n",
      "|04013|12.081175995|           5.1505854956|176.06725999999998|\n",
      "|06037|10.059485018|           13.691354033|133.71699999999998|\n",
      "|06059|8.3365759293|           10.249305431|158.62313999999998|\n",
      "|17031| 9.972268492|            7.028651671|174.98887000000002|\n",
      "|53061|6.2076401725|           3.5068929756|         112.52509|\n",
      "|04013|12.081175995|           5.1505854956|176.06725999999998|\n",
      "|06037|10.059485018|           13.691354033|133.71699999999998|\n",
      "+-----+------------+-----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "access2_query = '''\n",
    "SELECT fips, p_unins as perc_unins, p_not_eng_prof as perc_not_eng_proficient, PCP_ratio\n",
    "FROM access\n",
    "'''\n",
    "\n",
    "access_df = spark.sql(access2_query)\n",
    "access_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_df.createOrReplaceTempView('access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+---------------+---------------+------------------------+\n",
      "| fips|perc_adult_diab|perc_adult_obes|life_expectancy|perc_fair_or_poor_health|\n",
      "+-----+---------------+---------------+---------------+------------------------+\n",
      "|53061|            8.2|           28.9|   80.374691011|            14.403971122|\n",
      "|53061|            8.2|           28.9|   80.374691011|            14.403971122|\n",
      "|53061|            8.2|           28.9|   80.374691011|            14.403971122|\n",
      "|17031|            9.5|             28|   79.415729611|            17.140851058|\n",
      "|53061|            8.2|           28.9|   80.374691011|            14.403971122|\n",
      "|06059|            8.5|           19.4|   83.105149802|            13.902377773|\n",
      "|17031|            9.5|             28|   79.415729611|            17.140851058|\n",
      "|53061|            8.2|           28.9|   80.374691011|            14.403971122|\n",
      "|04013|            8.9|           26.9|   80.379854898|            15.622654225|\n",
      "|06037|            8.4|           21.7|   82.291130635|            16.418669345|\n",
      "|06059|            8.5|           19.4|   83.105149802|            13.902377773|\n",
      "|17031|            9.5|             28|   79.415729611|            17.140851058|\n",
      "|53061|            8.2|           28.9|   80.374691011|            14.403971122|\n",
      "|04013|            8.9|           26.9|   80.379854898|            15.622654225|\n",
      "|06037|            8.4|           21.7|   82.291130635|            16.418669345|\n",
      "|06059|            8.5|           19.4|   83.105149802|            13.902377773|\n",
      "|17031|            9.5|             28|   79.415729611|            17.140851058|\n",
      "|53061|            8.2|           28.9|   80.374691011|            14.403971122|\n",
      "|04013|            8.9|           26.9|   80.379854898|            15.622654225|\n",
      "|06037|            8.4|           21.7|   82.291130635|            16.418669345|\n",
      "+-----+---------------+---------------+---------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "health_query = '''\n",
    "SELECT fips, \n",
    "percent_adults_with_diabetes as perc_adult_diab,  percent_adults_with_obesity as perc_adult_obes, life_expectancy, \n",
    "percent_fair_or_poor_health as perc_fair_or_poor_health\n",
    "FROM covid1\n",
    "'''\n",
    "health_df = spark.sql(health_query)\n",
    "health_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "health_df.createOrReplaceTempView('health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+-----------------+------------------+\n",
      "| fips|perc_vacc|perc_smokers|perc_insuff_sleep|perc_phys_inactive|\n",
      "+-----+---------+------------+-----------------+------------------+\n",
      "|53061|       49|12.962308599|     34.783820371|              17.8|\n",
      "|53061|       49|12.962308599|     34.783820371|              17.8|\n",
      "|53061|       49|12.962308599|     34.783820371|              17.8|\n",
      "|17031|       45|13.776183162|      33.40278161|                21|\n",
      "|53061|       49|12.962308599|     34.783820371|              17.8|\n",
      "|06059|       46| 9.311197445|     30.787560897|              15.2|\n",
      "|17031|       45|13.776183162|      33.40278161|                21|\n",
      "|53061|       49|12.962308599|     34.783820371|              17.8|\n",
      "|04013|       47|13.686398251|     34.267942269|              20.4|\n",
      "|06037|       38|10.847678109|     35.147280534|              16.4|\n",
      "|06059|       46| 9.311197445|     30.787560897|              15.2|\n",
      "|17031|       45|13.776183162|      33.40278161|                21|\n",
      "|53061|       49|12.962308599|     34.783820371|              17.8|\n",
      "|04013|       47|13.686398251|     34.267942269|              20.4|\n",
      "|06037|       38|10.847678109|     35.147280534|              16.4|\n",
      "|06059|       46| 9.311197445|     30.787560897|              15.2|\n",
      "|17031|       45|13.776183162|      33.40278161|                21|\n",
      "|53061|       49|12.962308599|     34.783820371|              17.8|\n",
      "|04013|       47|13.686398251|     34.267942269|              20.4|\n",
      "|06037|       38|10.847678109|     35.147280534|              16.4|\n",
      "+-----+---------+------------+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lifestyle_query = '''\n",
    "SELECT fips, percent_vaccinated as perc_vacc, percent_smokers as perc_smokers, \n",
    "percent_insufficient_sleep as perc_insuff_sleep, percent_physically_inactive as perc_phys_inactive\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "lifestyle_df = spark.sql(lifestyle_query)\n",
    "lifestyle_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lifestyle_df.createOrReplaceTempView('lifestyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------------+------------+------------+------------+---------------+\n",
      "| fips|     p_black|p_amind_alaskan|     p_asian|    p_haw_pi|      p_hisp|p_nonhisp_white|\n",
      "+-----+------------+---------------+------------+------------+------------+---------------+\n",
      "|53061|3.3812696266|   1.5801919497| 11.63711911|0.6894088975|10.444090755|   68.847995033|\n",
      "|53061|3.3812696266|   1.5801919497| 11.63711911|0.6894088975|10.444090755|   68.847995033|\n",
      "|53061|3.3812696266|   1.5801919497| 11.63711911|0.6894088975|10.444090755|   68.847995033|\n",
      "|17031|23.041088947|   0.7384046267|7.8786130972|0.0747612245|25.538438137|    42.08236552|\n",
      "|53061|3.3812696266|   1.5801919497| 11.63711911|0.6894088975|10.444090755|   68.847995033|\n",
      "|06059|1.6403805688|   1.0328101224|21.382575092|0.3893008342|34.157624935|   40.128431924|\n",
      "|17031|23.041088947|   0.7384046267|7.8786130972|0.0747612245|25.538438137|    42.08236552|\n",
      "|53061|3.3812696266|   1.5801919497| 11.63711911|0.6894088975|10.444090755|   68.847995033|\n",
      "|04013|5.4661441944|   2.8070492044|4.5977350264|0.2905126117|31.278441398|   54.902054582|\n",
      "|06037|7.9644902913|   1.4373731262|15.356897093|0.3681157166|48.639634307|   26.117988212|\n",
      "|06059|1.6403805688|   1.0328101224|21.382575092|0.3893008342|34.157624935|   40.128431924|\n",
      "|17031|23.041088947|   0.7384046267|7.8786130972|0.0747612245|25.538438137|    42.08236552|\n",
      "|53061|3.3812696266|   1.5801919497| 11.63711911|0.6894088975|10.444090755|   68.847995033|\n",
      "|04013|5.4661441944|   2.8070492044|4.5977350264|0.2905126117|31.278441398|   54.902054582|\n",
      "|06037|7.9644902913|   1.4373731262|15.356897093|0.3681157166|48.639634307|   26.117988212|\n",
      "|06059|1.6403805688|   1.0328101224|21.382575092|0.3893008342|34.157624935|   40.128431924|\n",
      "|17031|23.041088947|   0.7384046267|7.8786130972|0.0747612245|25.538438137|    42.08236552|\n",
      "|53061|3.3812696266|   1.5801919497| 11.63711911|0.6894088975|10.444090755|   68.847995033|\n",
      "|04013|5.4661441944|   2.8070492044|4.5977350264|0.2905126117|31.278441398|   54.902054582|\n",
      "|06037|7.9644902913|   1.4373731262|15.356897093|0.3681157166|48.639634307|   26.117988212|\n",
      "+-----+------------+---------------+------------+------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic1_query = '''\n",
    "SELECT  fips, \n",
    "percent_black as p_black, percent_american_indian_alaska_native as p_amind_alaskan, \n",
    "percent_asian as p_asian, percent_native_hawaiian_other_pacific_islander as p_haw_pi, \n",
    "percent_hispanic as p_hisp, percent_non_hispanic_white as p_nonhisp_white\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "demographic1_df = spark.sql(demographic1_query)\n",
    "demographic1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-----------------+---------------+\n",
      "| fips| perc_female|segregation_index|perc_minorities|\n",
      "+-----+------------+-----------------+---------------+\n",
      "|53061|49.827402347|     31.003422819|             28|\n",
      "|53061|49.827402347|     31.003422819|             28|\n",
      "|53061|49.827402347|     31.003422819|             28|\n",
      "|17031|51.431630156|     51.804258492|           57.1|\n",
      "|53061|49.827402347|     31.003422819|             28|\n",
      "|06059|50.633496633|       32.9623503|             58|\n",
      "|17031|51.431630156|     51.804258492|           57.1|\n",
      "|53061|49.827402347|     31.003422819|             28|\n",
      "|04013|50.537178541|     32.393747135|           43.1|\n",
      "|06037|50.701240649|     35.079426177|           73.3|\n",
      "|06059|50.633496633|       32.9623503|             58|\n",
      "|17031|51.431630156|     51.804258492|           57.1|\n",
      "|53061|49.827402347|     31.003422819|             28|\n",
      "|04013|50.537178541|     32.393747135|           43.1|\n",
      "|06037|50.701240649|     35.079426177|           73.3|\n",
      "|06059|50.633496633|       32.9623503|             58|\n",
      "|17031|51.431630156|     51.804258492|           57.1|\n",
      "|53061|49.827402347|     31.003422819|             28|\n",
      "|04013|50.537178541|     32.393747135|           43.1|\n",
      "|06037|50.701240649|     35.079426177|           73.3|\n",
      "+-----+------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic2_query = '''\n",
    "SELECT fips, \n",
    "percent_female as perc_female, \n",
    "segregation_index_2 as segregation_index, percent_minorities as perc_minorities\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "demographic2_df = spark.sql(demographic2_query)\n",
    "demographic2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demographic2_df.createOrReplaceTempView('demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| fips|mean_temp|\n",
      "+-----+---------+\n",
      "|53061|     44.1|\n",
      "|53061|     44.8|\n",
      "|53061|     49.9|\n",
      "|17031|     36.1|\n",
      "|53061|     51.5|\n",
      "|06059|     57.7|\n",
      "|17031|     34.1|\n",
      "|53061|     47.1|\n",
      "|04013|     59.1|\n",
      "|06037|     58.2|\n",
      "|06059|     58.5|\n",
      "|17031|     32.2|\n",
      "|53061|     47.4|\n",
      "|04013|     61.7|\n",
      "|06037|     65.8|\n",
      "|06059|     58.5|\n",
      "|17031|     31.1|\n",
      "|53061|     47.3|\n",
      "|04013|     61.4|\n",
      "|06037|       67|\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "111364\n"
     ]
    }
   ],
   "source": [
    "weather_query = '''\n",
    "SELECT  fips, mean_temp\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "weather_df = spark.sql(weather_query)\n",
    "weather_df.show()\n",
    "print (weather_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_df.createOrReplaceTempView('weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+--------------------+\n",
      "| fips|km_to_closest_station|        station_name|\n",
      "+-----+---------------------+--------------------+\n",
      "|53061|    36.57287978476159|ARLINGTON MUNICIP...|\n",
      "|53061|    36.57287978476159|ARLINGTON MUNICIP...|\n",
      "|53061|    36.57287978476159|ARLINGTON MUNICIP...|\n",
      "|17031|    8.063384077043175|CHICAGO MIDWAY AI...|\n",
      "|53061|    36.57287978476159|ARLINGTON MUNICIP...|\n",
      "|06059|   10.047935596454208|SANTA ANA JOHN WA...|\n",
      "|17031|    8.063384077043175|CHICAGO MIDWAY AI...|\n",
      "|53061|    36.57287978476159|ARLINGTON MUNICIP...|\n",
      "|04013|     12.6083300580037|PHOENIX GOODYEAR ...|\n",
      "|06037|   21.174822877776425|LOS ANGELES WHITE...|\n",
      "|06059|   10.047935596454208|SANTA ANA JOHN WA...|\n",
      "|17031|    8.063384077043175|CHICAGO MIDWAY AI...|\n",
      "|53061|    36.57287978476159|ARLINGTON MUNICIP...|\n",
      "|04013|     12.6083300580037|PHOENIX GOODYEAR ...|\n",
      "|06037|   21.174822877776425|LOS ANGELES WHITE...|\n",
      "|06059|   10.047935596454208|SANTA ANA JOHN WA...|\n",
      "|17031|    8.063384077043175|CHICAGO MIDWAY AI...|\n",
      "|53061|    36.57287978476159|ARLINGTON MUNICIP...|\n",
      "|04013|     12.6083300580037|PHOENIX GOODYEAR ...|\n",
      "|06037|   21.174822877776425|LOS ANGELES WHITE...|\n",
      "+-----+---------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "111364\n"
     ]
    }
   ],
   "source": [
    "station_query = '''\n",
    "SELECT  fips, \n",
    "km_to_closest_station, station_name\n",
    "FROM covid1\n",
    "'''\n",
    "\n",
    "station_df = spark.sql(station_query)\n",
    "station_df.show()\n",
    "print (weather_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_df.createOrReplaceTempView('station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21040"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_df = spark.sql(\"SELECT station_name, fips FROM station WHERE station_name NOT LIKE '%AIR%' AND station_name NOT LIKE '%FIELD%' AND fips NOT LIKE '25025' AND fips NOT LIKE '06075' AND fips NOT LIKE '48029' AND fips NOT LIKE '06067'\")\n",
    "\n",
    "airport_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas_airport_df = airport_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NA                                   1815\n",
       "SCHENECTADY, NY US                    236\n",
       "STAUNTON, VA US                       226\n",
       "NEWTON 5 ENE, MS US                   202\n",
       "HENRY CO, TN US                       197\n",
       "THOMASVILLE 2 S, AL US                196\n",
       "YANKTON 2 E, SD US                    188\n",
       "BINGHAMTON GREATER AP, NY US          188\n",
       "CONCORDIA ASOS, KS US                 176\n",
       "CHARLOTTESVILLE 2 SSE, VA US          175\n",
       "OXFORD UNIVERSITY, MS US              168\n",
       "ASHEVILLE 8 SSW, NC US                156\n",
       "BEDFORD 5 WNW, IN US                  154\n",
       "MONTICELLO SULLIVAN, NY US            151\n",
       "BATESVILLE 8 WNW, AR US               150\n",
       "STROM THURMOND DAM, US                148\n",
       "SABRE AHP, TN US                      144\n",
       "KNOXVILLE DOWNTOWN ISLAND, TN US      139\n",
       "ELKINS 21 ENE, WV US                  134\n",
       "OAK RIDGE ASOS, TN US                 126\n",
       "LAWRENCE SMITH MEM, MO US             124\n",
       "FORT STEWART WRIGHT, GA US            123\n",
       "TOPEKA ASOS, KS US                    122\n",
       "MONROE WALTON CO, GA US               121\n",
       "MUSCLE SHOALS 2 N, AL US              120\n",
       "CAMPBELL CO, TN US                    117\n",
       "KENNETT MEM, MO US                    115\n",
       "COLUMBUS, NE US                       111\n",
       "LA VETA PASS, CO US                   111\n",
       "DARRINGTON 21 NNE, WA US              110\n",
       "                                     ... \n",
       "JOPLIN 24 N, MO US                     25\n",
       "WORTHINGTON 2 NNE, MN US               24\n",
       "KEWAUNEE, WI US                        23\n",
       "GOLF SIDNEY, NE US                     23\n",
       "PORT ORFORD, OR US                     23\n",
       "FIVE FINGER AK, US                     23\n",
       "COOS BAY 8 SW, OR US                   23\n",
       "KILO HARLOWTOWN, MT US                 23\n",
       "YORK RIVER E RANGE, VA US              22\n",
       "ULYSSES, KS US                         22\n",
       "CALCASIEU PASS, LA US                  22\n",
       "KEATON BEACH, FL US                    22\n",
       "SCOTT CITY MUNICIPAL, KS US            22\n",
       "FALLON NAAS, NV US                     21\n",
       "BLYTHEVILLE AFB, AR US                 20\n",
       "TOKE POINT, WA US                      19\n",
       "CHEYENNE MOUNTAIN AS, CO US            18\n",
       "GRINNELL, IA US                        17\n",
       "DETROIT LAKES, MN US                   16\n",
       "CLINES CORNERS, NM US                  16\n",
       "BRIDGEPORT SONORA JUNCTION, CA US      16\n",
       "BIG BAY, MI US                         13\n",
       "YOSEMITE VILLAGE 12 W, CA US           13\n",
       "SITKA 1 NE, AK US                       8\n",
       "BRONTE 11 NNE, TX US                    6\n",
       "LYONS RICE CO MUNICIPAL, KS US          6\n",
       "PINEY ISLAND, NC US                     5\n",
       "MARSHALL MEM MUNICIPAL, MO US           3\n",
       "PANTHER JUNCTION 2 N, TX US             3\n",
       "WHITMAN 5 ENE, NE US                    2\n",
       "Name: station_name, dtype: int64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_airport_df['station_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+---------------------------+\n",
      "| fips|date_stay_at_home_announced|date_stay_at_home_effective|\n",
      "+-----+---------------------------+---------------------------+\n",
      "|53061|                 2020-03-23|                 2020-03-23|\n",
      "|53061|                 2020-03-23|                 2020-03-23|\n",
      "|53061|                 2020-03-23|                 2020-03-23|\n",
      "|17031|                 2020-03-20|                 2020-03-21|\n",
      "|53061|                 2020-03-23|                 2020-03-23|\n",
      "|06059|                 2020-03-19|                 2020-03-19|\n",
      "|17031|                 2020-03-20|                 2020-03-21|\n",
      "|53061|                 2020-03-23|                 2020-03-23|\n",
      "|04013|                 2020-03-30|                 2020-03-31|\n",
      "|06037|                 2020-03-19|                 2020-03-19|\n",
      "|06059|                 2020-03-19|                 2020-03-19|\n",
      "|17031|                 2020-03-20|                 2020-03-21|\n",
      "|53061|                 2020-03-23|                 2020-03-23|\n",
      "|04013|                 2020-03-30|                 2020-03-31|\n",
      "|06037|                 2020-03-19|                 2020-03-19|\n",
      "|06059|                 2020-03-19|                 2020-03-19|\n",
      "|17031|                 2020-03-20|                 2020-03-21|\n",
      "|53061|                 2020-03-23|                 2020-03-23|\n",
      "|04013|                 2020-03-30|                 2020-03-31|\n",
      "|06037|                 2020-03-19|                 2020-03-19|\n",
      "+-----+---------------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stay_home_query = '''\n",
    "SELECT  fips, \n",
    "date_stay_at_home_announced, date_stay_at_home_effective\n",
    "FROM covid1\n",
    "'''\n",
    "stay_home_df = spark.sql(stay_home_query)\n",
    "stay_home_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Can't extract value from county#2229; line 4 pos 22\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.sql.\n: org.apache.spark.sql.AnalysisException: Can't extract value from county#2229; line 4 pos 22\n\tat org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$3.apply(LogicalPlan.scala:253)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$3.apply(LogicalPlan.scala:252)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:252)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:148)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$5$$anonfun$31.apply(Analyzer.scala:603)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$5$$anonfun$31.apply(Analyzer.scala:603)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:603)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:599)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:301)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$5.apply(QueryPlan.scala:209)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:209)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:599)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:540)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:540)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:477)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:51)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n\tat sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-269-e6d2faac21e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mOUTER\u001b[0m \u001b[0mJOIN\u001b[0m \u001b[0mfinancial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mON\u001b[0m \u001b[0mcounty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinancial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \"\"\")\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtable1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Can't extract value from county#2229; line 4 pos 22\""
     ]
    }
   ],
   "source": [
    "table1 = spark.sql(\"\"\"SELECT *\n",
    "                FROM county\n",
    "                   OUTER JOIN financial\n",
    "                   ON county.fips = financial.fips\n",
    "                \"\"\")\n",
    "\n",
    "table1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table2 = spark.sql(\"\"\"SELECT *\n",
    "                FROM age\n",
    "                   INNER JOIN \n",
    "                   ON county.fips = financial.fips\n",
    "                \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
